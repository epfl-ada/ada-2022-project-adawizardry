{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>plot</th>\n",
       "      <th>similarity</th>\n",
       "      <th>release_date</th>\n",
       "      <th>event</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23141</th>\n",
       "      <td>The Founding of a Republic</td>\n",
       "      <td>In 1945 after the end of the Second Sino-Japan...</td>\n",
       "      <td>0.604864</td>\n",
       "      <td>2009</td>\n",
       "      <td>Negotiations_to_end_apartheid_in_South_Africa</td>\n",
       "      <td>[Chinese Movies, Drama, War film, World cinema]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39420</th>\n",
       "      <td>The Deluge</td>\n",
       "      <td>The film is set in the 17th century during the...</td>\n",
       "      <td>0.572665</td>\n",
       "      <td>1974</td>\n",
       "      <td>Negotiations_to_end_apartheid_in_South_Africa</td>\n",
       "      <td>[War film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6928</th>\n",
       "      <td>The Founding of a Party</td>\n",
       "      <td>During the early 20th century, China is marked...</td>\n",
       "      <td>0.572192</td>\n",
       "      <td>2011</td>\n",
       "      <td>Negotiations_to_end_apartheid_in_South_Africa</td>\n",
       "      <td>[Drama, Historical fiction]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39032</th>\n",
       "      <td>7 Man Army</td>\n",
       "      <td>During the Second Sino- Japanese war 20,000 Ja...</td>\n",
       "      <td>0.564577</td>\n",
       "      <td>1976</td>\n",
       "      <td>Negotiations_to_end_apartheid_in_South_Africa</td>\n",
       "      <td>[Chinese Movies, Martial Arts Film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27370</th>\n",
       "      <td>The Final Inch</td>\n",
       "      <td>The film depicts the problems still occurring ...</td>\n",
       "      <td>0.559969</td>\n",
       "      <td>2009</td>\n",
       "      <td>Negotiations_to_end_apartheid_in_South_Africa</td>\n",
       "      <td>[Documentary, Short Film]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name  \\\n",
       "23141  The Founding of a Republic   \n",
       "39420                  The Deluge   \n",
       "6928      The Founding of a Party   \n",
       "39032                  7 Man Army   \n",
       "27370              The Final Inch   \n",
       "\n",
       "                                                    plot similarity  \\\n",
       "23141  In 1945 after the end of the Second Sino-Japan...   0.604864   \n",
       "39420  The film is set in the 17th century during the...   0.572665   \n",
       "6928   During the early 20th century, China is marked...   0.572192   \n",
       "39032  During the Second Sino- Japanese war 20,000 Ja...   0.564577   \n",
       "27370  The film depicts the problems still occurring ...   0.559969   \n",
       "\n",
       "      release_date                                          event  \\\n",
       "23141         2009  Negotiations_to_end_apartheid_in_South_Africa   \n",
       "39420         1974  Negotiations_to_end_apartheid_in_South_Africa   \n",
       "6928          2011  Negotiations_to_end_apartheid_in_South_Africa   \n",
       "39032         1976  Negotiations_to_end_apartheid_in_South_Africa   \n",
       "27370         2009  Negotiations_to_end_apartheid_in_South_Africa   \n",
       "\n",
       "                                                genres  \n",
       "23141  [Chinese Movies, Drama, War film, World cinema]  \n",
       "39420                                       [War film]  \n",
       "6928                       [Drama, Historical fiction]  \n",
       "39032              [Chinese Movies, Martial Arts Film]  \n",
       "27370                        [Documentary, Short Film]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>plot</th>\n",
       "      <th>similarity</th>\n",
       "      <th>release_date</th>\n",
       "      <th>event</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35867</th>\n",
       "      <td>Titan A.E.</td>\n",
       "      <td>In 3028, the Drej, a malevolent, energy-based...</td>\n",
       "      <td>0.494681</td>\n",
       "      <td>2000</td>\n",
       "      <td>Apollo_13</td>\n",
       "      <td>[Action, Adventure, Animation, Family Film, Sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35867</th>\n",
       "      <td>Titan A.E.</td>\n",
       "      <td>In 3028, the Drej, a malevolent, energy-based...</td>\n",
       "      <td>0.457863</td>\n",
       "      <td>2000</td>\n",
       "      <td>Sputnik_2</td>\n",
       "      <td>[Action, Adventure, Animation, Family Film, Sc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name                                               plot  \\\n",
       "35867  Titan A.E.   In 3028, the Drej, a malevolent, energy-based...   \n",
       "35867  Titan A.E.   In 3028, the Drej, a malevolent, energy-based...   \n",
       "\n",
       "      similarity release_date      event  \\\n",
       "35867   0.494681         2000  Apollo_13   \n",
       "35867   0.457863         2000  Sputnik_2   \n",
       "\n",
       "                                                  genres  \n",
       "35867  [Action, Adventure, Animation, Family Film, Sc...  \n",
       "35867  [Action, Adventure, Animation, Family Film, Sc...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_similar_df = pd.read_pickle(\"similarity_df.pkl\")\n",
    "allevents = top_similar_df['event'].unique()\n",
    "display(top_similar_df.head(5))\n",
    "apollo= top_similar_df[top_similar_df['event']=='Apollo_11']\n",
    "display(apollo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all events '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Negotiations_to_end_apartheid_in_South_Africa', 'Nelson_Mandela',\n",
       "       'Fall_of_the_Berlin_Wall', 'Stanislav_Petrov', 'Falklands_War',\n",
       "       'Attempted_assassination_of_Ronald_Reagan#:~:text=On%20March%2030%2C%201981%2C%20President,engagement%20at%20the%20Washington%20Hilton.',\n",
       "       'Iranian_Revolution#:~:text=Iranian%20people%20voted%20in%20a,it%20created%20throughout%20the%20world.',\n",
       "       'Apollo_13', 'Apollo_11',\n",
       "       'Death_and_state_funeral_of_Winston_Churchill',\n",
       "       'Civil_Rights_Act_of_1964', 'Assassination_of_John_F._Kennedy',\n",
       "       'Berlin_Wall', 'Vietnam_War', 'Great_Leap_Forward', 'Sputnik_2',\n",
       "       'Hungarian_Revolution_of_1956', 'Warsaw_Pact',\n",
       "       'Brown_v._Board_of_Education', 'Korean_War', 'Marshall_Plan',\n",
       "       'Cold_War', 'Truman_Doctrine',\n",
       "       'Atomic_bombings_of_Hiroshima_and_Nagasaki',\n",
       "       'Attack_on_Pearl_Harbor'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>plot</th>\n",
       "      <th>similarity</th>\n",
       "      <th>release_date</th>\n",
       "      <th>event</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23141</th>\n",
       "      <td>The Founding of a Republic</td>\n",
       "      <td>In 1945 after the end of the Second Sino-Japan...</td>\n",
       "      <td>0.604864</td>\n",
       "      <td>2009</td>\n",
       "      <td>Negotiations_to_end_apartheid_in_South_Africa</td>\n",
       "      <td>[Chinese Movies, Drama, War film, World cinema]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39420</th>\n",
       "      <td>The Deluge</td>\n",
       "      <td>The film is set in the 17th century during the...</td>\n",
       "      <td>0.572665</td>\n",
       "      <td>1974</td>\n",
       "      <td>Negotiations_to_end_apartheid_in_South_Africa</td>\n",
       "      <td>[War film]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name  \\\n",
       "23141  The Founding of a Republic   \n",
       "39420                  The Deluge   \n",
       "\n",
       "                                                    plot similarity  \\\n",
       "23141  In 1945 after the end of the Second Sino-Japan...   0.604864   \n",
       "39420  The film is set in the 17th century during the...   0.572665   \n",
       "\n",
       "      release_date                                          event  \\\n",
       "23141         2009  Negotiations_to_end_apartheid_in_South_Africa   \n",
       "39420         1974  Negotiations_to_end_apartheid_in_South_Africa   \n",
       "\n",
       "                                                genres  \n",
       "23141  [Chinese Movies, Drama, War film, World cinema]  \n",
       "39420                                       [War film]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m display(top_similar_df\u001b[39m.\u001b[39mhead(\u001b[39m2\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mdistilbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m BertForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mdistilbert-base-uncased\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculateSentiment\u001b[39m(text):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m## Referenced this medium article : https://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m## Used the https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english from Huggingface \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     chunksize \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/modeling_utils.py:2276\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2273\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2275\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2276\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2278\u001b[0m \u001b[39mif\u001b[39;00m load_in_8bit:\n\u001b[1;32m   2279\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbitsandbytes\u001b[39;00m \u001b[39mimport\u001b[39;00m get_keys_to_not_convert, replace_8bit_linear\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1526\u001b[0m, in \u001b[0;36mBertForSequenceClassification.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_labels \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mnum_labels\n\u001b[1;32m   1524\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m-> 1526\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert \u001b[39m=\u001b[39m BertModel(config)\n\u001b[1;32m   1527\u001b[0m classifier_dropout \u001b[39m=\u001b[39m (\n\u001b[1;32m   1528\u001b[0m     config\u001b[39m.\u001b[39mclassifier_dropout \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mclassifier_dropout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m config\u001b[39m.\u001b[39mhidden_dropout_prob\n\u001b[1;32m   1529\u001b[0m )\n\u001b[1;32m   1530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(classifier_dropout)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:891\u001b[0m, in \u001b[0;36mBertModel.__init__\u001b[0;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[1;32m    890\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings \u001b[39m=\u001b[39m BertEmbeddings(config)\n\u001b[0;32m--> 891\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m BertEncoder(config)\n\u001b[1;32m    893\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39m=\u001b[39m BertPooler(config) \u001b[39mif\u001b[39;00m add_pooling_layer \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[39m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:559\u001b[0m, in \u001b[0;36mBertEncoder.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    558\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m--> 559\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([BertLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:559\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    558\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m--> 559\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([BertLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:482\u001b[0m, in \u001b[0;36mBertLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrossattention \u001b[39m=\u001b[39m BertAttention(config, position_embedding_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mabsolute\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    481\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate \u001b[39m=\u001b[39m BertIntermediate(config)\n\u001b[0;32m--> 482\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m BertOutput(config)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:458\u001b[0m, in \u001b[0;36mBertOutput.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[1;32m    457\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 458\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(config\u001b[39m.\u001b[39;49mintermediate_size, config\u001b[39m.\u001b[39;49mhidden_size)\n\u001b[1;32m    459\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_eps)\n\u001b[1;32m    460\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mhidden_dropout_prob)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "display(\"all events \", allevents)\n",
    "display(top_similar_df.head(2))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def calculateSentiment(text):\n",
    "    ## Referenced this medium article : https://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f\n",
    "    ## Used the https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english from Huggingface \n",
    "    \n",
    "    chunksize = 512\n",
    "    tokens = tokenizer.encode_plus(text, add_special_tokens=False,\n",
    "                                return_tensors='pt')\n",
    "\n",
    "    input_id_chunks = list(tokens['input_ids'][0].split(510))\n",
    "    mask_chunks = list(tokens['attention_mask'][0].split(510))\n",
    "\n",
    "    # loop through each chunk\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        # add CLS and SEP tokens to input IDs\n",
    "        input_id_chunks[i] = torch.cat([\n",
    "            torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
    "        ])\n",
    "        # add attention tokens to attention mask\n",
    "        mask_chunks[i] = torch.cat([\n",
    "            torch.tensor([1]), mask_chunks[i], torch.tensor([1])\n",
    "        ])\n",
    "        # get required padding length\n",
    "        pad_len = chunksize - input_id_chunks[i].shape[0]\n",
    "        # check if tensor length satisfies required chunk size\n",
    "        if pad_len > 0:\n",
    "            # if padding length is more than 0, we must add padding\n",
    "            input_id_chunks[i] = torch.cat([\n",
    "                input_id_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "            mask_chunks[i] = torch.cat([\n",
    "                mask_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "\n",
    "    # # check length of each tensor\n",
    "    # for chunk in input_id_chunks:\n",
    "    #     print(len(chunk))\n",
    "\n",
    "    ##  Now the final step of placing our tensors back into the dictionary style format we had before.\n",
    "    input_ids = torch.stack(input_id_chunks)\n",
    "    attention_mask = torch.stack(mask_chunks)\n",
    "\n",
    "    input_dict = {\n",
    "        'input_ids': input_ids.long(),\n",
    "        'attention_mask': attention_mask.int()\n",
    "    }\n",
    "    input_dict\n",
    "\n",
    "    ## run the model\n",
    "    outputs = model(**input_dict)\n",
    "    probs = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "    probs = probs.mean(dim=0)\n",
    "    winner = torch.argmax(probs).item()\n",
    "\n",
    "\n",
    "    return (probs, winner)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'original shape'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2500, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'less shape'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1576, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m display(\u001b[39m\"\u001b[39m\u001b[39mless shape\u001b[39m\u001b[39m\"\u001b[39m , top_similar_df_less\u001b[39m.\u001b[39mshape) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# atomic_df = top_similar_df_less[top_similar_df_less['event'] == 'Atomic_bombings_of_Hiroshima_and_Nagasaki']\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# berlin_df = top_similar_df_less[top_similar_df_less['event'] == 'Nelson_Mandela']\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# display(atomic_df.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# display(berlin_df.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m top_similar_df_less[\u001b[39m'\u001b[39m\u001b[39mprobability\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m top_similar_df_less[\u001b[39m'\u001b[39;49m\u001b[39mplot\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: calculateSentiment(x)[\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1079\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> 1082\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1132\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1138\u001b[0m             values,\n\u001b[1;32m   1139\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1140\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1141\u001b[0m         )\n\u001b[1;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1144\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb Cell 3\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m display(\u001b[39m\"\u001b[39m\u001b[39mless shape\u001b[39m\u001b[39m\"\u001b[39m , top_similar_df_less\u001b[39m.\u001b[39mshape) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# atomic_df = top_similar_df_less[top_similar_df_less['event'] == 'Atomic_bombings_of_Hiroshima_and_Nagasaki']\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# berlin_df = top_similar_df_less[top_similar_df_less['event'] == 'Nelson_Mandela']\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# display(atomic_df.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# display(berlin_df.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m top_similar_df_less[\u001b[39m'\u001b[39m\u001b[39mprobability\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m top_similar_df_less[\u001b[39m'\u001b[39m\u001b[39mplot\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: calculateSentiment(x)[\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32m/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb Cell 3\u001b[0m in \u001b[0;36mcalculateSentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m input_dict\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m## run the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minput_dict)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(outputs[\u001b[39m0\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gladys/Desktop/ada-2022-project-adawizardry/sentiment.ipynb#W6sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m probs \u001b[39m=\u001b[39m probs\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1566\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1558\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1563\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1566\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1567\u001b[0m     input_ids,\n\u001b[1;32m   1568\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1569\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1570\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1571\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1572\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1573\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1574\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1575\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1576\u001b[0m )\n\u001b[1;32m   1578\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1580\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1021\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1012\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1014\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1015\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1016\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1020\u001b[0m )\n\u001b[0;32m-> 1021\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1022\u001b[0m     embedding_output,\n\u001b[1;32m   1023\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1024\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1025\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1026\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1027\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1028\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1029\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1030\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1031\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1032\u001b[0m )\n\u001b[1;32m   1033\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1034\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    611\u001b[0m         hidden_states,\n\u001b[1;32m    612\u001b[0m         attention_mask,\n\u001b[1;32m    613\u001b[0m         layer_head_mask,\n\u001b[1;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    616\u001b[0m         past_key_value,\n\u001b[1;32m    617\u001b[0m         output_attentions,\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:496\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    485\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    486\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    494\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    495\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    497\u001b[0m         hidden_states,\n\u001b[1;32m    498\u001b[0m         attention_mask,\n\u001b[1;32m    499\u001b[0m         head_mask,\n\u001b[1;32m    500\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    501\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    502\u001b[0m     )\n\u001b[1;32m    503\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    505\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:426\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    417\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    418\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    425\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 426\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    427\u001b[0m         hidden_states,\n\u001b[1;32m    428\u001b[0m         attention_mask,\n\u001b[1;32m    429\u001b[0m         head_mask,\n\u001b[1;32m    430\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    431\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    432\u001b[0m         past_key_value,\n\u001b[1;32m    433\u001b[0m         output_attentions,\n\u001b[1;32m    434\u001b[0m     )\n\u001b[1;32m    435\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    436\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:308\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(hidden_states))\n\u001b[0;32m--> 308\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue(hidden_states))\n\u001b[1;32m    310\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[1;32m    312\u001b[0m use_cache \u001b[39m=\u001b[39m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "display(\"original shape\" , top_similar_df.shape) \n",
    "top_similar_df_less = top_similar_df.groupby('event').apply(\n",
    "    lambda x: x.loc[\n",
    "        x['similarity'] > 0.5 ] \n",
    ").reset_index(drop=True)\n",
    "\n",
    "top_similar_df_less = top_similar_df.groupby('event').apply(\n",
    "    lambda x: x.loc[\n",
    "        x['similarity'] < 0.512 ] \n",
    ").reset_index(drop=True)\n",
    "\n",
    "display(\"less shape\" , top_similar_df_less.shape) \n",
    "\n",
    "# atomic_df = top_similar_df_less[top_similar_df_less['event'] == 'Atomic_bombings_of_Hiroshima_and_Nagasaki']\n",
    "# berlin_df = top_similar_df_less[top_similar_df_less['event'] == 'Nelson_Mandela']\n",
    "# display(atomic_df.shape)\n",
    "# display(berlin_df.shape)\n",
    "\n",
    "top_similar_df_less['probability'] = top_similar_df_less['plot'].apply(lambda x: calculateSentiment(x)[1])\n",
    "# atomic_df['probability'] = atomic_df['plot'].apply(lambda x: calculateSentiment(x)[0])\n",
    "# berlin_df['probability'] = berlin_df['plot'].apply(lambda x: calculateSentiment(x)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>plot</th>\n",
       "      <th>similarity</th>\n",
       "      <th>release_date</th>\n",
       "      <th>event</th>\n",
       "      <th>genres</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Dream is Alive</td>\n",
       "      <td>The movie includes scenes from numerous shuttl...</td>\n",
       "      <td>0.653769</td>\n",
       "      <td>1985</td>\n",
       "      <td>Apollo_11</td>\n",
       "      <td>[Documentary, Short Film]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apollo 13</td>\n",
       "      <td>On July 20, 1969, veteran astronaut Jim Lovell...</td>\n",
       "      <td>0.681827</td>\n",
       "      <td>1995</td>\n",
       "      <td>Apollo_13</td>\n",
       "      <td>[Adventure, Drama, History]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dream is Alive</td>\n",
       "      <td>The movie includes scenes from numerous shuttl...</td>\n",
       "      <td>0.661487</td>\n",
       "      <td>1985</td>\n",
       "      <td>Apollo_13</td>\n",
       "      <td>[Documentary, Short Film]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JFK</td>\n",
       "      <td>The film's opening encompasses newsreel footag...</td>\n",
       "      <td>0.682642</td>\n",
       "      <td>1991</td>\n",
       "      <td>Assassination_of_John_F._Kennedy</td>\n",
       "      <td>[Biography, Drama, History, Mystery, Thriller]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hiroshima</td>\n",
       "      <td>The film opens in April 1945 with the death of...</td>\n",
       "      <td>0.685578</td>\n",
       "      <td>1995</td>\n",
       "      <td>Atomic_bombings_of_Hiroshima_and_Nagasaki</td>\n",
       "      <td>[Drama, Japanese Movies, Period piece, Politic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Countdown to Looking Glass</td>\n",
       "      <td>The film begins with a fictional broadcast fro...</td>\n",
       "      <td>0.656042</td>\n",
       "      <td>1984</td>\n",
       "      <td>Cold_War</td>\n",
       "      <td>[Drama, Political thriller, War film]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We Were Soldiers</td>\n",
       "      <td>{{Plot}} A French unit is on patrol in Vietnam...</td>\n",
       "      <td>0.694403</td>\n",
       "      <td>2002</td>\n",
       "      <td>Korean_War</td>\n",
       "      <td>[Action, ActionAdventure, Combat Films, Drama,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Countdown to Looking Glass</td>\n",
       "      <td>The film begins with a fictional broadcast fro...</td>\n",
       "      <td>0.663181</td>\n",
       "      <td>1984</td>\n",
       "      <td>Korean_War</td>\n",
       "      <td>[Drama, Political thriller, War film]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Heaven's Soldiers</td>\n",
       "      <td>The film begins with high-level military leade...</td>\n",
       "      <td>0.662264</td>\n",
       "      <td>2005</td>\n",
       "      <td>Korean_War</td>\n",
       "      <td>[Action, Comedy, Science Fiction]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Countdown to Looking Glass</td>\n",
       "      <td>The film begins with a fictional broadcast fro...</td>\n",
       "      <td>0.697152</td>\n",
       "      <td>1984</td>\n",
       "      <td>Vietnam_War</td>\n",
       "      <td>[Drama, Political thriller, War film]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  \\\n",
       "0          The Dream is Alive   \n",
       "1                   Apollo 13   \n",
       "2          The Dream is Alive   \n",
       "3                         JFK   \n",
       "4                   Hiroshima   \n",
       "5  Countdown to Looking Glass   \n",
       "6            We Were Soldiers   \n",
       "7  Countdown to Looking Glass   \n",
       "8           Heaven's Soldiers   \n",
       "9  Countdown to Looking Glass   \n",
       "\n",
       "                                                plot similarity release_date  \\\n",
       "0  The movie includes scenes from numerous shuttl...   0.653769         1985   \n",
       "1  On July 20, 1969, veteran astronaut Jim Lovell...   0.681827         1995   \n",
       "2  The movie includes scenes from numerous shuttl...   0.661487         1985   \n",
       "3  The film's opening encompasses newsreel footag...   0.682642         1991   \n",
       "4  The film opens in April 1945 with the death of...   0.685578         1995   \n",
       "5  The film begins with a fictional broadcast fro...   0.656042         1984   \n",
       "6  {{Plot}} A French unit is on patrol in Vietnam...   0.694403         2002   \n",
       "7  The film begins with a fictional broadcast fro...   0.663181         1984   \n",
       "8  The film begins with high-level military leade...   0.662264         2005   \n",
       "9  The film begins with a fictional broadcast fro...   0.697152         1984   \n",
       "\n",
       "                                       event  \\\n",
       "0                                  Apollo_11   \n",
       "1                                  Apollo_13   \n",
       "2                                  Apollo_13   \n",
       "3           Assassination_of_John_F._Kennedy   \n",
       "4  Atomic_bombings_of_Hiroshima_and_Nagasaki   \n",
       "5                                   Cold_War   \n",
       "6                                 Korean_War   \n",
       "7                                 Korean_War   \n",
       "8                                 Korean_War   \n",
       "9                                Vietnam_War   \n",
       "\n",
       "                                              genres  probability  \n",
       "0                          [Documentary, Short Film]            0  \n",
       "1                        [Adventure, Drama, History]            0  \n",
       "2                          [Documentary, Short Film]            0  \n",
       "3     [Biography, Drama, History, Mystery, Thriller]            0  \n",
       "4  [Drama, Japanese Movies, Period piece, Politic...            0  \n",
       "5              [Drama, Political thriller, War film]            0  \n",
       "6  [Action, ActionAdventure, Combat Films, Drama,...            0  \n",
       "7              [Drama, Political thriller, War film]            0  \n",
       "8                  [Action, Comedy, Science Fiction]            0  \n",
       "9              [Drama, Political thriller, War film]            0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_similar_df_less.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2f359080f89aa17f8dd37ea509bbd04ae9046c3b5da43c511c9e33295f4eb62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
